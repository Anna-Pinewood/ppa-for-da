{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nltk_text_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kS0TtTK4sFx4"
      },
      "source": [
        "##Предобработка текста с NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gW0xp0A9miQZ"
      },
      "source": [
        "Обработка естественного языка (англ. Natural language processing, **NLP**) - это область, в которой основное внимание уделяется тому, чтобы сделать естественный, человеческий язык пригодным для использования компьютерными программами. **NLTK** (англ. Natural Language Toolkit - набор инструментов для естественного языка) - это пакет Python, который вы можете использовать для решения задач NLP.\n",
        "\n",
        "Многие данные, которые вы могли бы анализировать, являются неструктурированными и содержат текст понятный человеку. Прежде чем вы сможете программно анализировать эти данные, вам сначала нужно их обработать. В даннорм туториале будет рассмотрено применение библиотеки NLTK для предобработки текстовых данных.\n",
        "\n",
        "Установим и импортируем библиотеку NLTK."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l55aA3b6CfA2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7241b5a2-66bc-445e-9326-f4e92fbe64f0"
      },
      "source": [
        "! pip install nltk\n",
        "import nltk"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVkKlpsys6JT"
      },
      "source": [
        "#Токенизация\n",
        "\n",
        "Токенизация - это разбиение текста на меньшие части. Это позволяет вам работать с небольшими фрагментами текста, которые остаются относительно связными и значимыми даже вне контекста остального текста. Таким образом данные становятся более структурированными, и в дальнейшем их легче анализировать.\n",
        "\n",
        "NLTK содержит несколько видов токенизаторов: по словам, предложениям или регулярным выражениям. Полный список можно найти [здесь](https://www.nltk.org/api/nltk.tokenize.html)\n",
        "\n",
        "\n",
        "Для токенизации текста необходимо предварительно загрузить подпакет punkt. NLTK поставляется с большим количеством исходных данных, предобученных моделей, грамматик и т.д. Полный список можно найти [здесь](https://www.nltk.org/nltk_data/). Все эти дополнения можно получить воспользовавшись функией downloads. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVHOhcAR_eaZ",
        "outputId": "128e2113-cea5-4fc2-fbda-f9165cbd8276"
      },
      "source": [
        "nltk.download('punkt')\n",
        "\n",
        "# функция sent_tokenize - для разделения текста по предложениям\n",
        "text = \"Предложение. Предложение, которое содержит запятую. Восклицательный знак! Вопрос?\" \n",
        "sents = nltk.sent_tokenize(text)\n",
        "print(sents)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "['Предложение.', 'Предложение, которое содержит запятую.', 'Восклицательный знак!', 'Вопрос?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OADd6hK7AsWH",
        "outputId": "8531324d-0dc9-41e9-eaaa-ef312e77f6d6"
      },
      "source": [
        "# функция word_tokenize - для разделения текста по словам\n",
        "from nltk import word_tokenize\n",
        "sent = \"В этом предложении есть много слов, мы их разделим.\"\n",
        "print(word_tokenize(sent))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['В', 'этом', 'предложении', 'есть', 'много', 'слов', ',', 'мы', 'их', 'разделим', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYua4AlRAi0P"
      },
      "source": [
        "#Стоп слова\n",
        "\n",
        "Существует некоторый список слов, встречающихся довольно часто, и зачастую эти слова не несут большой информативной нагрузки. Такие слова являются шумом для последующего анализа. Библиотека NLTK имеет несколько наборов стоп-слов (в том числе для русского языка), которые предварительно необходимо загрузить."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cloKuNEgA2bQ",
        "outputId": "07b38d48-fcdc-4175-bd5d-6058205bab22"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stop_words=set(stopwords.words('russian'))\n",
        "print(stop_words)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "{'этот', 'их', 'что', 'всю', 'про', 'где', 'же', 'ведь', 'вот', 'чтобы', 'бы', 'себе', 'почти', 'какая', 'иногда', 'через', 'этого', 'нибудь', 'опять', 'о', 'мой', 'вдруг', 'на', 'всего', 'вас', 'после', 'то', 'чего', 'сам', 'теперь', 'не', 'есть', 'да', 'только', 'тот', 'во', 'него', 'с', 'ней', 'мы', 'нее', 'ну', 'такой', 'перед', 'между', 'ей', 'со', 'тем', 'наконец', 'этой', 'у', 'все', 'уж', 'была', 'будто', 'еще', 'ним', 'я', 'ничего', 'они', 'быть', 'чем', 'если', 'или', 'три', 'зачем', 'за', 'чтоб', 'без', 'ее', 'себя', 'эту', 'раз', 'вам', 'вы', 'хоть', 'тут', 'к', 'было', 'от', 'один', 'там', 'кто', 'нас', 'уже', 'ни', 'были', 'потому', 'здесь', 'том', 'и', 'когда', 'тоже', 'по', 'тебя', 'а', 'много', 'им', 'в', 'тогда', 'ему', 'куда', 'них', 'он', 'моя', 'но', 'для', 'хорошо', 'нет', 'даже', 'нельзя', 'до', 'того', 'разве', 'свою', 'может', 'ж', 'конечно', 'впрочем', 'чуть', 'более', 'под', 'совсем', 'сейчас', 'больше', 'об', 'ты', 'будет', 'его', 'из', 'два', 'как', 'потом', 'этом', 'всех', 'надо', 'никогда', 'какой', 'другой', 'можно', 'ли', 'эти', 'над', 'меня', 'при', 'мне', 'был', 'лучше', 'она', 'так', 'всегда'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5GBpNWD37rh"
      },
      "source": [
        "Удалить стоп-слова (предварительно осуществим токенизацию) можно следующим образом:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkbFIqqB2EBE",
        "outputId": "5cead293-9aa7-48fb-d3c9-30a2cf21af58"
      },
      "source": [
        "from nltk import word_tokenize\n",
        "tokens = word_tokenize(\"В этом предложении есть много слов, мы их разделим.\")\n",
        "# список отфильтрованных слов\n",
        "filtered_tokens = []\n",
        "stop_words = set(stopwords.words('russian'))\n",
        "for token in tokens:\n",
        "  if token not in stop_words:\n",
        "    filtered_tokens.append(token)\n",
        "# Количество слов в предложении до и после токенизации\n",
        "print(len(tokens), len(filtered_tokens))\n",
        "# Предложение после токенизации\n",
        "print(filtered_tokens)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11 6\n",
            "['В', 'предложении', 'слов', ',', 'разделим', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLpSZVO77YEG"
      },
      "source": [
        "Поскольку набор стоп-слов - это список, то к нему можно добавить дополнительные слова или, наоборот, удалить из него те, которые будут информативными для вашего случая."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nqj84JHvBeYG"
      },
      "source": [
        "#Стемминг\n",
        "\n",
        "Стемминг - это задача обработки текста, в которой вы сокращаете слова до их корня, который является основной частью слова. Выделение основы позволяет сконцентрировать внимание на основном значении слова. NLTK предоставляет большое количество стеммеров, подходящих для разных языков. Полный список можно найти [здесь](https://www.nltk.org/howto/stem.html). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCqwkAXcC2nm",
        "outputId": "71ea397a-09c9-4a62-c09b-3a97f7b4f993"
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "porter_stemmer = PorterStemmer()\n",
        "print(porter_stemmer.stem(\"crying\"))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cri\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-XwIoaiC36P",
        "outputId": "fb369235-4e07-419b-d790-487ff0a3392d"
      },
      "source": [
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "lancaster_stemmer = LancasterStemmer()\n",
        "print(lancaster_stemmer.stem(\"crying\"))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cry\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geusbyuq8AUX"
      },
      "source": [
        "Стеммер с поддержкой русского языка - Snowball Stemmer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cm9hCMSr8Ua2",
        "outputId": "82a1245e-5c60-4eeb-b7ac-ab0b56769a65"
      },
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "snowball = SnowballStemmer(language=\"russian\")\n",
        "print(snowball.stem(\"Хороший\"), snowball.stem(\"Хорошая\"))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "хорош хорош\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afYxcMIG8uYv"
      },
      "source": [
        "Проблемы могут возникнуть со словами, которые значительно изменяются в других формах"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzHm5rRU8yhL",
        "outputId": "a30b0a4a-070c-4071-fc62-443800f2c836"
      },
      "source": [
        "print(snowball.stem(\"Хочу\"), snowball.stem(\"Хотеть\"))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "хоч хотет\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hnqhUm288fe"
      },
      "source": [
        "*Хотеть* и *хочу* — грамматические формы одного и то же слова, но стемминг обрубает окончания согласно своему алгоритму. Поэтому возможно следует применить другой метод — лемматизацию."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIs4py5EC9yQ"
      },
      "source": [
        "#Лемматизация\n",
        "\n",
        "Лемматизация - процесс предобработки текста при котором над словом проводится морфологический анализ и выявляется его начальная форма. Лемматизация поддерживается в NLTK только для английского языка. Кроме слова, к которому применяется лемматизация, необходимо указать также часть речи: \"n\" для существительных (значение по умолчанию), \"v\" для глаголов, \"a\" для прилагательных, \"r\" для наречий и \"s\" для спутниковых прилагательных."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZE14PZwVDGrF",
        "outputId": "18c85330-9b16-446b-99ad-aefad3c5ee80"
      },
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "print(wordnet_lemmatizer.lemmatize(\"went\", pos=\"v\"))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "go\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZI7QN_lEOvn",
        "outputId": "7a974971-a830-4ba3-9ee5-3bf3cd96f7b7"
      },
      "source": [
        "# по умолчанию все слова будут рассматриваться как существительные\n",
        "# ничего не произойдёт для глагола\n",
        "print(wordnet_lemmatizer.lemmatize(\"went\"))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "went\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHFFyp-oEZ-A",
        "outputId": "4da5b7f1-5eb1-4e66-adf3-5855054990d8"
      },
      "source": [
        "print(wordnet_lemmatizer.lemmatize(\"dogs\"))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DC_28xogE1GW"
      },
      "source": [
        "Для лемматизации слов на русском языке следует воспользоваться другими пакетами, например pymorphy2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkAOQA7lFI8l"
      },
      "source": [
        "#Заключение\n",
        "\n",
        "В данном туториале продемонстрированы основные методики предобработки текстовых данных. Следует отметить, что список этапов предобработки текста и их последовательность зависят от конкретной задачи и должны определяться исследователем. \n",
        "\n",
        "Некоторые из методик, например, определение частей речи (Tagging Parts of Speech, полезно перед лемматизацией) или распознавание именовых сущностей (NER) и т.д. не были рассмотрены, знакомство с ними остаётся на откуп читателю."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyehCiqvFQ6n"
      },
      "source": [
        "#Источники\n",
        "\n",
        "1.   [Документация](https://www.nltk.org/index.html)\n",
        "2.   [https://neerc.ifmo.ru/wiki/index.php?title=Обработка_естественного_языка](https://neerc.ifmo.ru/wiki/index.php?title=%D0%9E%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B0_%D0%B5%D1%81%D1%82%D0%B5%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D0%BE%D0%B3%D0%BE_%D1%8F%D0%B7%D1%8B%D0%BA%D0%B0)\n",
        "3.   [Natural Language Processing With Python's NLTK Package](https://realpython.com/nltk-nlp-python/#getting-started-with-pythons-nltk)\n",
        "4.   [Предобработка текста в NLP](https://medium.com/@bigdataschool/предобработка-текста-в-nlp-82c164bb7416)\n",
        "5.   [Wiki Обработка естественного языка](https://ru.wikipedia.org/wiki/Обработка_естественного_языка)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
      
      "source": [
        "#Ревью\n",
        "Пример четко выражает концепцию токена, который может быть сгенерирован из библиотеки NLTK, он также выражает концепцию слова «Стоп», где он указывает, что не все слова имеют отношение к делу или слова информативного характера, также выражается концепция «основание». возможность обрабатывать тексты в их корне, что важно, например, для русского языка. Также указывается концепция "стемминга", которая является альтернативой концепции стемминга. Кроме того, в руководстве объясняется, что Пример не следует хронологическому порядку русского языка и переходит к анализу слов на английском языке, поэтому цель учебного пособия не ясна, хотя, если изучаемые концепции понятны четко, возможно, вам следует развить идею до тех пор, пока достижение конечной цели.\n",
       
      
      ]
      
      
      
      
    }
  ]
  
}
