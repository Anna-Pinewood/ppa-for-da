{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Предобработка_текстовых_данных.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Установка дополнительных зависимостей:"
      ],
      "metadata": {
        "id": "xETB3XDtwEb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install contractions \n",
        "!pip install inflect\n",
        "!pip install num2words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVc9eAp_eT37",
        "outputId": "57bfc8e4-520a-4b94-b76a-8781fdff67a7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.68-py2.py3-none-any.whl (8.1 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting anyascii\n",
            "  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n",
            "\u001b[K     ,████████████████████████████████, 284 kB 37.4 MB/s \n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[K     ,████████████████████████████████, 106 kB 56.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.0 contractions-0.1.68 pyahocorasick-1.4.4 textsearch-0.0.21\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.7/dist-packages (2.1.0)\n",
            "Collecting num2words\n",
            "  Downloading num2words-0.5.10-py3-none-any.whl (101 kB)\n",
            "\u001b[K     ,████████████████████████████████, 101 kB 10.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.7/dist-packages (from num2words) (0.6.2)\n",
            "Installing collected packages: num2words\n",
            "Successfully installed num2words-0.5.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Импортирование библиотек:"
      ],
      "metadata": {
        "id": "5TFCE5nKwzJ7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sx2bAhRnUGMP",
        "outputId": "4c232ba2-e888-4baa-8a4f-c38d8fd20ac8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    , \n",
            "[nltk_data]    , Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    ,   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    , Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    ,   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    , Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    ,   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    , Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    ,   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    , Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    ,   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    , Downloading package movie_reviews to\n",
            "[nltk_data]    ,     /root/nltk_data...\n",
            "[nltk_data]    ,   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    , Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    ,   Unzipping corpora/names.zip.\n",
            "[nltk_data]    , Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    ,   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    , Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    ,   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    , Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    ,   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    , Downloading package twitter_samples to\n",
            "[nltk_data]    ,     /root/nltk_data...\n",
            "[nltk_data]    ,   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    , Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    ,   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    , Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    ,   Unzipping corpora/omw-1.4.zip.\n",
            "[nltk_data]    , Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    ,   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    , Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    ,   Unzipping corpora/wordnet2021.zip.\n",
            "[nltk_data]    , Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    ,   Unzipping corpora/wordnet31.zip.\n",
            "[nltk_data]    , Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    ,   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    , Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    ,   Unzipping corpora/words.zip.\n",
            "[nltk_data]    , Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    ,     /root/nltk_data...\n",
            "[nltk_data]    ,   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    , Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    ,   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    , Downloading package snowball_data to\n",
            "[nltk_data]    ,     /root/nltk_data...\n",
            "[nltk_data]    , Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    ,     /root/nltk_data...\n",
            "[nltk_data]    ,   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    , \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import re, string, unicodedata\n",
        "import nltk\n",
        "import contractions\n",
        "import inflect\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
        "from num2words import num2words\n",
        "import nltk\n",
        "nltk.download(\"popular\")\n",
        "# nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Токенизация — это самый первый шаг при обработке текста. Заключается в разбиении (разделении) длинных строк текста в более мелкие: абзацы делим на предложения, предложения на слова"
      ],
      "metadata": {
        "id": "4eHjTc6eoUJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The striped bats are hanging on their feet for best\"\n",
        "words = nltk.word_tokenize(text)\n",
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVEwYi76a4WZ",
        "outputId": "82896ca6-119f-451d-b407-bade4c2b75b0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'striped',\n",
              " 'bats',\n",
              " 'are',\n",
              " 'hanging',\n",
              " 'on',\n",
              " 'their',\n",
              " 'feet',\n",
              " 'for',\n",
              " 'best']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Нормализация**"
      ],
      "metadata": {
        "id": "TL5sazS8ukoz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Приведение слов к нижнему регистру"
      ],
      "metadata": {
        "id": "XknCi1Vm0Ihe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_lowercase(words):\n",
        "    return [word.lower() for word in words]\n",
        "words = to_lowercase(words)\n",
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hujthrYouzi8",
        "outputId": "dae9c41e-cb2a-4045-b743-d4e433283dba"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " 'striped',\n",
              " 'bats',\n",
              " 'are',\n",
              " 'hanging',\n",
              " 'on',\n",
              " 'their',\n",
              " 'feet',\n",
              " 'for',\n",
              " 'best']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Удаленеи знаков препинания"
      ],
      "metadata": {
        "id": "DfgKG57b0Fkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(words):\n",
        "    return [re.sub(r'[^\\w\\s]', '', word) for word in words if re.sub(r'[^\\w\\s]', '', word) != '']\n",
        "words = remove_punctuation(words)\n",
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDsgf8KK0P2P",
        "outputId": "83ed123d-eabf-4a3d-c9d8-11db9c1f02f6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " 'striped',\n",
              " 'bats',\n",
              " 'are',\n",
              " 'hanging',\n",
              " 'on',\n",
              " 'their',\n",
              " 'feet',\n",
              " 'for',\n",
              " 'best']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Заменить числовое представление текстовым"
      ],
      "metadata": {
        "id": "lPQPkmzb0oGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_numbers(words):\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word.isdigit():\n",
        "            new_word = num2words(word, lang='ru')\n",
        "            new_words.append(new_word)\n",
        "        else:\n",
        "            new_words.append(word)\n",
        "\n",
        "    return new_words\n",
        "words = replace_numbers(words)\n",
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LC3gO9Fo0rx0",
        "outputId": "2b3677be-8a2c-4ff0-c6cc-6b3dfa6f51f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " 'striped',\n",
              " 'bats',\n",
              " 'are',\n",
              " 'hanging',\n",
              " 'on',\n",
              " 'their',\n",
              " 'feet',\n",
              " 'for',\n",
              " 'best']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Стемминг(нахождение основ слова)"
      ],
      "metadata": {
        "id": "rjtqiePd2AJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stem_words(words):\n",
        "    stemmer = LancasterStemmer()\n",
        "    stems = []\n",
        "    for word in words:\n",
        "        stem = stemmer.stem(word)\n",
        "        stems.append(stem)\n",
        "    return stems\n",
        "words = stem_words(words)\n",
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0NRm_4r2Crb",
        "outputId": "f35990d8-5d3d-4126-d357-de898c78b7cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the', 'striped', 'bat', 'ar', 'hang', 'on', 'their', 'feet', 'for', 'best']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Лемматизация глаголов"
      ],
      "metadata": {
        "id": "7Y40_fXj3zmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_verbs(words):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = []\n",
        "    for word in words:\n",
        "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
        "        lemmas.append(lemma)\n",
        "    return lemmas\n",
        "words = lemmatize_verbs(words)\n",
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ct57tSa3zyR",
        "outputId": "bbb0da91-0dd7-46f6-d9ac-5712c919be13"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the', 'strip', 'bat', 'be', 'hang', 'on', 'their', 'feet', 'for', 'best']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ревью Олег Стоцкий: заменил to_lowercase, remove_punctuation более компактными вариантами\n"
      ],
      "metadata": {
        "id": "5yoBhtLl_Nnk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "A_z4FPT3-pO1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}