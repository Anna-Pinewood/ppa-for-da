{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Natasha.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esDsQB0cvmBM"
      },
      "source": [
        "Natasha решает базовые задачи NLP для русского языка: токенизация, сегментация предложений, встраивание слов, морфологиz, лемматизация, нормализация фраз, синтаксический анализ, работа с NER, извлечение фактов. \n",
        "\n",
        "Natasha объединяет библиотеки проекта Natasha под одним удобным API:\n",
        "\n",
        "Razdel - токенизация, сегментация предложений для русского языка\n",
        "Navec - вектора русский слов\n",
        "Slovnet - библиотека с методами глубокого обучения для русского NLP, компактные модели для русской морфологии, синтаксиса и NER.\n",
        "Yargy - извлечение фактов на основе правил, аналогичное синтаксическому анализатору Tomita.\n",
        "\n",
        "Рассмотрим примеры использования"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEVp8ulQplGO",
        "outputId": "efab57cf-2acd-4ae4-ba3e-3787756d8177"
      },
      "source": [
        " !pip install natasha"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting natasha\n",
            "  Downloading natasha-1.4.0-py3-none-any.whl (34.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 34.4 MB 29 kB/s \n",
            "\u001b[?25hCollecting yargy>=0.14.0\n",
            "  Downloading yargy-0.15.0-py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 125 kB/s \n",
            "\u001b[?25hCollecting navec>=0.9.0\n",
            "  Downloading navec-0.10.0-py3-none-any.whl (23 kB)\n",
            "Collecting slovnet>=0.3.0\n",
            "  Downloading slovnet-0.5.0-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 6.9 MB/s \n",
            "\u001b[?25hCollecting ipymarkup>=0.8.0\n",
            "  Downloading ipymarkup-0.9.0-py3-none-any.whl (14 kB)\n",
            "Collecting razdel>=0.5.0\n",
            "  Downloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.7 MB/s \n",
            "\u001b[?25hCollecting intervaltree>=3\n",
            "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from navec>=0.9.0->natasha) (1.19.5)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2->natasha) (0.6.2)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 43.7 MB/s \n",
            "\u001b[?25hCollecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Building wheels for collected packages: intervaltree\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26119 sha256=19cf4576d537b227d6dad86876afe28062fbaf8acbf26950273d1c83968fa177\n",
            "  Stored in directory: /root/.cache/pip/wheels/16/85/bd/1001cbb46dcfb71c2001cd7401c6fb250392f22a81ce3722f7\n",
            "Successfully built intervaltree\n",
            "Installing collected packages: pymorphy2-dicts-ru, dawg-python, razdel, pymorphy2, navec, intervaltree, yargy, slovnet, ipymarkup, natasha\n",
            "  Attempting uninstall: intervaltree\n",
            "    Found existing installation: intervaltree 2.1.0\n",
            "    Uninstalling intervaltree-2.1.0:\n",
            "      Successfully uninstalled intervaltree-2.1.0\n",
            "Successfully installed dawg-python-0.7.2 intervaltree-3.1.0 ipymarkup-0.9.0 natasha-1.4.0 navec-0.10.0 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 razdel-0.5.0 slovnet-0.5.0 yargy-0.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PM4FX4IaqhKv"
      },
      "source": [
        "from natasha import (\n",
        "    Segmenter,\n",
        "    MorphVocab,\n",
        "    \n",
        "    NewsEmbedding,\n",
        "    NewsMorphTagger,\n",
        "    NewsSyntaxParser,\n",
        "    NewsNERTagger,\n",
        "    \n",
        "    PER,\n",
        "    NamesExtractor,\n",
        "\n",
        "    Doc\n",
        ")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqLHCMrxqnSI"
      },
      "source": [
        "segmenter = Segmenter()\n",
        "morph_vocab = MorphVocab()\n",
        "\n",
        "emb = NewsEmbedding()\n",
        "morph_tagger = NewsMorphTagger(emb)\n",
        "syntax_parser = NewsSyntaxParser(emb)\n",
        "ner_tagger = NewsNERTagger(emb)\n",
        "\n",
        "names_extractor = NamesExtractor(morph_vocab)\n",
        "\n",
        "text = '''Студенты и сотрудники лаборатории Машинного обучения Университета ИТМО разработали библиотеку для Python, которая решает ключевую задачу машинного обучения.\n",
        "\n",
        "Расскажем, почему появился этот инструмент и что он умеет.\n",
        "Нехватка алгоритмов\n",
        "\n",
        "Одна из ключевых задач машинного обучения — снижение размерности данных. Дата-саентисты сокращают число переменных, вычленяя среди них значения, наибольшим образом влияющие на результат. После этой операции модель машинного обучения требует меньше памяти, работает быстрее и качественнее. Пример ниже показывает, что исключение дублирующих признаков увеличивает точность классификации с 0,903 до 0,943.\n",
        "\n",
        "Существует два подхода к уменьшению размерности — конструирование и выбор признаков. В областях вроде биоинформатики и медицины чаще используют последний, так как он позволяет выделить значимые признаки с сохранением семантики, то есть не меняет исходный смысл признаков. Однако в самых распространенных библиотеках машинного обучения на Python — scikit-learn, pytorch, keras, tensorflow — нет полноценного набора методов выбора признаков.\n",
        "\n",
        "Для решения этой проблемы студенты и аспиранты Университета ИТМО разработали открытую библиотеку — ITMO_FS. Над ней трудится команда под руководством Ивана Сметанникова, доцента факультета информационных технологий и программирования, заместителя заведующего лабораторией Машинного обучения. Ведущий разработчик — Никита Пильненьский, закончивший магистратуру «Машинное обучение и анализ данных». Теперь он поступает в аспирантуру.\n",
        "\n",
        "«За последние несколько лет в нашу лабораторию приходили запросы на решение задач, для которых не подходил стандартный инструментарий. Например, нам требовались ансамблирующие алгоритмы на основе объединения фильтров, или алгоритмы, учитывающие наличие заранее известных (экспертно-размеченных) значимых признаков.\n",
        "\n",
        "Посмотрев на существующие решения, мы пришли к выводу, что они не только не содержат необходимые нам инструменты, но и не являются достаточно гибкими для их возможной мягкой интеграции. В контексте того, что среди таких библиотек конкуренция слаба, мы решили сделать свою библиотеку, исправляющую большинство недостатков».\n",
        "\n",
        "— Иван Сметанников\n",
        "'''\n",
        "doc = Doc(text)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocdnCUWXrD5z"
      },
      "source": [
        "# Разделение"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urLQeWiTrIK8"
      },
      "source": [
        "После сегментации можно работать как с отдельными токенами, так и с целыми предложениями (которые в себе также содержат токены)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JWiFLPcqtDp",
        "outputId": "945d22ae-89d0-4451-f42f-c40b88295ae7"
      },
      "source": [
        "doc.segment(segmenter)\n",
        "print(doc.tokens[:5])\n",
        "print(doc.sents[:5])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DocToken(stop=8, text='Студенты'), DocToken(start=9, stop=10, text='и'), DocToken(start=11, stop=21, text='сотрудники'), DocToken(start=22, stop=33, text='лаборатории'), DocToken(start=34, stop=43, text='Машинного')]\n",
            "[DocSent(stop=156, text='Студенты и сотрудники лаборатории Машинного обуче..., tokens=[...]), DocSent(start=158, stop=216, text='Расскажем, почему появился этот инструмент и что ..., tokens=[...]), DocSent(start=217, stop=310, text='Нехватка алгоритмов\\n\\nОдна из ключевых задач маш..., tokens=[...]), DocSent(start=311, stop=424, text='Дата-саентисты сокращают число переменных, вычлен..., tokens=[...]), DocSent(start=425, stop=526, text='После этой операции модель машинного обучения тре..., tokens=[...])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1pYrG_0rglP"
      },
      "source": [
        "# Морфология"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMMpCQzTrit_"
      },
      "source": [
        "Библиотека умеет проводить морфологический анализ, можно работать сново с каждым токеном. Также уже есть встроенный \"красивый\" вывод признаков\n",
        "\n",
        "Зависит от шага с разделением и дополняет уже существующую информацию о токенах"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2q2v7hyrjeA",
        "outputId": "4ebfb0d3-ce09-45da-9284-8e9ea34d4821"
      },
      "source": [
        "doc.tag_morph(morph_tagger)\n",
        "print(doc.tokens[:5])\n",
        "doc.sents[0].morph.print()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DocToken(stop=8, text='Студенты', pos='NOUN', feats=<Anim,Nom,Masc,Plur>), DocToken(start=9, stop=10, text='и', pos='CCONJ'), DocToken(start=11, stop=21, text='сотрудники', pos='NOUN', feats=<Anim,Nom,Masc,Plur>), DocToken(start=22, stop=33, text='лаборатории', pos='NOUN', feats=<Inan,Gen,Fem,Sing>), DocToken(start=34, stop=43, text='Машинного', pos='ADJ', feats=<Gen,Pos,Neut,Sing>)]\n",
            "            Студенты NOUN|Animacy=Anim|Case=Nom|Gender=Masc|Number=Plur\n",
            "                   и CCONJ\n",
            "          сотрудники NOUN|Animacy=Anim|Case=Nom|Gender=Masc|Number=Plur\n",
            "         лаборатории NOUN|Animacy=Inan|Case=Gen|Gender=Fem|Number=Sing\n",
            "           Машинного ADJ|Case=Gen|Degree=Pos|Gender=Neut|Number=Sing\n",
            "            обучения NOUN|Animacy=Inan|Case=Gen|Gender=Neut|Number=Sing\n",
            "        Университета PROPN|Animacy=Inan|Case=Gen|Gender=Masc|Number=Sing\n",
            "                ИТМО PROPN|Animacy=Inan|Case=Gen|Gender=Masc|Number=Sing\n",
            "         разработали VERB|Aspect=Perf|Mood=Ind|Number=Plur|Tense=Past|VerbForm=Fin|Voice=Act\n",
            "          библиотеку NOUN|Animacy=Inan|Case=Acc|Gender=Fem|Number=Sing\n",
            "                 для ADP\n",
            "              Python PROPN|Foreign=Yes\n",
            "                   , PUNCT\n",
            "             которая PRON|Case=Nom|Gender=Fem|Number=Sing\n",
            "              решает VERB|Aspect=Imp|Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\n",
            "            ключевую ADJ|Case=Acc|Degree=Pos|Gender=Fem|Number=Sing\n",
            "              задачу NOUN|Animacy=Inan|Case=Acc|Gender=Fem|Number=Sing\n",
            "           машинного ADJ|Case=Gen|Degree=Pos|Gender=Neut|Number=Sing\n",
            "            обучения NOUN|Animacy=Inan|Case=Gen|Gender=Neut|Number=Sing\n",
            "                   . PUNCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_DLAal4sJFO"
      },
      "source": [
        "# Лемматизация"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Roov1qssfo_"
      },
      "source": [
        "Зависит от шага с морфологическим разбором и дополняет уже существующую информацию о токенах\n",
        "\n",
        "Выведем первые 15 примеров приведение слова к его нормальной форме"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxrJaGuxspNo",
        "outputId": "2cba04eb-e38f-4e6f-ad85-7916c7634750"
      },
      "source": [
        "for token in doc.tokens:\n",
        "    token.lemmatize(morph_vocab)\n",
        "    \n",
        "print(doc.tokens[:5])\n",
        "{_.text: _.lemma for _ in doc.tokens[:15]}"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DocToken(stop=8, text='Студенты', pos='NOUN', feats=<Anim,Nom,Masc,Plur>, lemma='студент'), DocToken(start=9, stop=10, text='и', pos='CCONJ', lemma='и'), DocToken(start=11, stop=21, text='сотрудники', pos='NOUN', feats=<Anim,Nom,Masc,Plur>, lemma='сотрудник'), DocToken(start=22, stop=33, text='лаборатории', pos='NOUN', feats=<Inan,Gen,Fem,Sing>, lemma='лаборатория'), DocToken(start=34, stop=43, text='Машинного', pos='ADJ', feats=<Gen,Pos,Neut,Sing>, lemma='машинный')]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{',': ',',\n",
              " 'Python': 'python',\n",
              " 'ИТМО': 'итмо',\n",
              " 'Машинного': 'машинный',\n",
              " 'Студенты': 'студент',\n",
              " 'Университета': 'университет',\n",
              " 'библиотеку': 'библиотека',\n",
              " 'для': 'для',\n",
              " 'и': 'и',\n",
              " 'которая': 'который',\n",
              " 'лаборатории': 'лаборатория',\n",
              " 'обучения': 'обучение',\n",
              " 'разработали': 'разработать',\n",
              " 'решает': 'решать',\n",
              " 'сотрудники': 'сотрудник'}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tD8bMbIWtgUa"
      },
      "source": [
        "# Синтаксический разбор"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBbJIZiytj0g"
      },
      "source": [
        "Зависит от шага с разделением на токены, а именно использует информацию о предложениях"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cG0PwdXqts7O",
        "outputId": "3e68d443-3b52-429c-c9fe-4ccf93ca10c0"
      },
      "source": [
        "doc.parse_syntax(syntax_parser)\n",
        "print(doc.tokens[:5])\n",
        "doc.sents[0].syntax.print()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DocToken(stop=8, text='Студенты', id='1_1', head_id='1_9', rel='nsubj', pos='NOUN', feats=<Anim,Nom,Masc,Plur>, lemma='студент'), DocToken(start=9, stop=10, text='и', id='1_2', head_id='1_3', rel='cc', pos='CCONJ', lemma='и'), DocToken(start=11, stop=21, text='сотрудники', id='1_3', head_id='1_1', rel='conj', pos='NOUN', feats=<Anim,Nom,Masc,Plur>, lemma='сотрудник'), DocToken(start=22, stop=33, text='лаборатории', id='1_4', head_id='1_3', rel='nmod', pos='NOUN', feats=<Inan,Gen,Fem,Sing>, lemma='лаборатория'), DocToken(start=34, stop=43, text='Машинного', id='1_5', head_id='1_6', rel='amod', pos='ADJ', feats=<Gen,Pos,Neut,Sing>, lemma='машинный')]\n",
            "  ┌────────► Студенты     nsubj\n",
            "  │       ┌► и            cc\n",
            "  │     ┌─└─ сотрудники   \n",
            "  │ ┌─┌─└──► лаборатории  nmod\n",
            "  │ │ │   ┌► Машинного    amod\n",
            "  │ │ └──►└─ обучения     nmod\n",
            "  │ └────►┌─ Университета nmod\n",
            "  │       └► ИТМО         nmod\n",
            "┌─└───────┌─ разработали  \n",
            "│     ┌─┌─└► библиотеку   obj\n",
            "│     │ │ ┌► для          case\n",
            "│     │ └►└─ Python       nmod\n",
            "│     │ ┌──► ,            punct\n",
            "│     │ │ ┌► которая      nsubj\n",
            "│     └►└─└─ решает       acl:relcl\n",
            "│     │   ┌► ключевую     amod\n",
            "│   ┌─└──►└─ задачу       obj\n",
            "│   │     ┌► машинного    amod\n",
            "│   └────►└─ обучения     nmod\n",
            "└──────────► .            punct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b_g6txAt1nl"
      },
      "source": [
        "# Распознование именованных сущностей"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlNi5eFnt4tE"
      },
      "source": [
        "Зависит от шага с разделением на токены"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5NwlP6LuB6m",
        "outputId": "031a2557-6eb3-4411-8983-418d89f48374"
      },
      "source": [
        "doc.tag_ner(ner_tagger)\n",
        "print(doc.spans[:5])\n",
        "doc.ner.print()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DocSpan(start=34, stop=52, type='ORG', text='Машинного обучения', tokens=[...]), DocSpan(start=53, stop=70, type='ORG', text='Университета ИТМО', tokens=[...]), DocSpan(start=1130, stop=1147, type='ORG', text='Университета ИТМО', tokens=[...]), DocSpan(start=1233, stop=1251, type='PER', text='Ивана Сметанникова', tokens=[...]), DocSpan(start=1355, stop=1373, type='ORG', text='Машинного обучения', tokens=[...])]\n",
            "Студенты и сотрудники лаборатории Машинного обучения Университета ИТМО\n",
            "                                  ORG─────────────── ORG──────────────\n",
            " разработали библиотеку для Python, которая решает ключевую задачу \n",
            "машинного обучения.\n",
            "Расскажем, почему появился этот инструмент и что он умеет.\n",
            "Нехватка алгоритмов\n",
            "Одна из ключевых задач машинного обучения — снижение размерности \n",
            "данных. Дата-саентисты сокращают число переменных, вычленяя среди них \n",
            "значения, наибольшим образом влияющие на результат. После этой \n",
            "операции модель машинного обучения требует меньше памяти, работает \n",
            "быстрее и качественнее. Пример ниже показывает, что исключение \n",
            "дублирующих признаков увеличивает точность классификации с 0,903 до \n",
            "0,943.\n",
            "Существует два подхода к уменьшению размерности — конструирование и \n",
            "выбор признаков. В областях вроде биоинформатики и медицины чаще \n",
            "используют последний, так как он позволяет выделить значимые признаки \n",
            "с сохранением семантики, то есть не меняет исходный смысл признаков. \n",
            "Однако в самых распространенных библиотеках машинного обучения на \n",
            "Python — scikit-learn, pytorch, keras, tensorflow — нет полноценного \n",
            "набора методов выбора признаков.\n",
            "Для решения этой проблемы студенты и аспиранты Университета ИТМО \n",
            "                                               ORG────────────── \n",
            "разработали открытую библиотеку — ITMO_FS. Над ней трудится команда \n",
            "под руководством Ивана Сметанникова, доцента факультета информационных\n",
            "                 PER───────────────                                   \n",
            " технологий и программирования, заместителя заведующего лабораторией \n",
            "Машинного обучения. Ведущий разработчик — Никита Пильненьский, \n",
            "ORG───────────────                        PER────────────────  \n",
            "закончивший магистратуру «Машинное обучение и анализ данных». Теперь \n",
            "он поступает в аспирантуру.\n",
            "«За последние несколько лет в нашу лабораторию приходили запросы на \n",
            "решение задач, для которых не подходил стандартный инструментарий. \n",
            "Например, нам требовались ансамблирующие алгоритмы на основе \n",
            "объединения фильтров, или алгоритмы, учитывающие наличие заранее \n",
            "известных (экспертно-размеченных) значимых признаков.\n",
            "Посмотрев на существующие решения, мы пришли к выводу, что они не \n",
            "только не содержат необходимые нам инструменты, но и не являются \n",
            "достаточно гибкими для их возможной мягкой интеграции. В контексте \n",
            "того, что среди таких библиотек конкуренция слаба, мы решили сделать \n",
            "свою библиотеку, исправляющую большинство недостатков».\n",
            "— Иван Сметанников\n",
            "  PER─────────────\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzVMqHm7uOT1"
      },
      "source": [
        "Затем такие сущности можно нормализовать\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMtsX3VEuSZQ",
        "outputId": "af66afd3-50d5-4f08-8f43-aaf2c35d6fa9"
      },
      "source": [
        "for span in doc.spans:\n",
        "   span.normalize(morph_vocab)\n",
        "print(doc.spans[:5])\n",
        "{_.text: _.normal for _ in doc.spans if _.text != _.normal}"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DocSpan(start=34, stop=52, type='ORG', text='Машинного обучения', tokens=[...], normal='Машинное обучение'), DocSpan(start=53, stop=70, type='ORG', text='Университета ИТМО', tokens=[...], normal='Университет ИТМО'), DocSpan(start=1130, stop=1147, type='ORG', text='Университета ИТМО', tokens=[...], normal='Университет ИТМО'), DocSpan(start=1233, stop=1251, type='PER', text='Ивана Сметанникова', tokens=[...], normal='Иван Сметанников'), DocSpan(start=1355, stop=1373, type='ORG', text='Машинного обучения', tokens=[...], normal='Машинное обучение')]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Ивана Сметанникова': 'Иван Сметанников',\n",
              " 'Машинного обучения': 'Машинное обучение',\n",
              " 'Университета ИТМО': 'Университет ИТМО'}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiG6UODBubvP"
      },
      "source": [
        "А также рзделить на составные чати (имя, фамилия и тд)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwEtYGQzuhCe",
        "outputId": "89a5aa16-d69a-42f8-8f77-e2497af361cd"
      },
      "source": [
        "for span in doc.spans:\n",
        "   if span.type == PER:\n",
        "       span.extract_fact(names_extractor)\n",
        "\n",
        "print(doc.spans[:5])\n",
        "{_.normal: _.fact.as_dict for _ in doc.spans if _.type == PER}"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DocSpan(start=34, stop=52, type='ORG', text='Машинного обучения', tokens=[...], normal='Машинное обучение'), DocSpan(start=53, stop=70, type='ORG', text='Университета ИТМО', tokens=[...], normal='Университет ИТМО'), DocSpan(start=1130, stop=1147, type='ORG', text='Университета ИТМО', tokens=[...], normal='Университет ИТМО'), DocSpan(start=1233, stop=1251, type='PER', text='Ивана Сметанникова', tokens=[...], normal='Иван Сметанников', fact=DocFact(slots=[...])), DocSpan(start=1355, stop=1373, type='ORG', text='Машинного обучения', tokens=[...], normal='Машинное обучение')]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Иван Сметанников': {'first': 'Иван', 'last': 'Сметанников'},\n",
              " 'Никита Пильненьский': {'first': 'Никита', 'last': 'Пильненьский'}}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}